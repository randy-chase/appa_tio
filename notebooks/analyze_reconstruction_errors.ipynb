{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Appa: Autoencoder Reconstruction Error Analysis\n",
        "\n",
        "This notebook analyzes reconstruction errors from a trained Appa autoencoder by processing ERA5 data through the model and computing various error metrics.\n",
        "\n",
        "Based on the reconstruction.py script, this notebook provides an interactive way to:\n",
        "- Load ERA5 data and process it through the autoencoder\n",
        "- Compute reconstruction errors (MSE, signed errors)\n",
        "- Generate error histograms and statistics\n",
        "- Visualize error patterns across variables and spatial locations\n",
        "- Analyze error characteristics for different atmospheric conditions\n",
        "\n",
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from omegaconf import OmegaConf\n",
        "from einops import rearrange\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add the appa module to the path\n",
        "sys.path.append('/home/azureuser/cloudfiles/code/Users/randy.chase/appa_tio/')\n",
        "\n",
        "import appa\n",
        "from appa.save import load_auto_encoder\n",
        "from appa.data.datasets import ERA5Dataset\n",
        "from appa.data.dataloaders import get_dataloader\n",
        "from appa.data.transforms import StandardizeTransform\n",
        "from appa.data.const import (\n",
        "    CONTEXT_VARIABLES,\n",
        "    ERA5_ATMOSPHERIC_VARIABLES,\n",
        "    ERA5_PRESSURE_LEVELS,\n",
        "    ERA5_RESOLUTION,\n",
        "    ERA5_SURFACE_VARIABLES,\n",
        "    ERA5_VARIABLES,\n",
        "    SUB_PRESSURE_LEVELS,\n",
        ")\n",
        "from appa.config import PATH_ERA5, PATH_MASK, PATH_STAT\n",
        "from appa.config.hydra import compose\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Setup\n",
        "\n",
        "**IMPORTANT**: Before running this analysis, you need to download the ERA5 data. The Appa project uses WeatherBench2 data format.\n",
        "\n",
        "### Option 1: Download ERA5 Data Subset (Space-Efficient)\n",
        "\n",
        "For reconstruction analysis, you only need a small subset of the data. Here are space-efficient options:\n",
        "\n",
        "#### **Option 1A: Small Date Range (Recommended for Testing)**\n",
        "```bash\n",
        "# Download just 1 month of data (much smaller!)\n",
        "python scripts/data/download_era5.py \\\n",
        "    output_path=/home/azureuser/cloudfiles/code/Users/randy.chase/appa_data/data/era5_2019-01-01_2019-01-31.zarr \\\n",
        "    start_date=2019-01-01 \\\n",
        "    end_date=2019-01-31\n",
        "\n",
        "# Compute statistics for this subset\n",
        "python scripts/data/data_stats.py \\\n",
        "    data_path=/path/to/your/data/era5_2019-01-01_2019-01-31.zarr \\\n",
        "    output_path=/path/to/your/data/stats_era5_2019-01-01_2019-01-31.zarr \\\n",
        "    start_date=2019-01-01 \\\n",
        "    end_date=2019-01-31\n",
        "```\n",
        "\n",
        "#### **Option 1B: Even Smaller - Just Surface Variables**\n",
        "```bash\n",
        "# Download only surface variables (6 variables instead of 71)\n",
        "python scripts/data/download_era5.py \\\n",
        "    output_path=/path/to/your/data/era5_surface_only.zarr \\\n",
        "    start_date=2019-01-01 \\\n",
        "    end_date=2019-01-31 \\\n",
        "    variables=\"2m_temperature,10m_u_component_of_wind,10m_v_component_of_wind,mean_sea_level_pressure,total_precipitation,sea_surface_temperature\"\n",
        "```\n",
        "\n",
        "#### **Option 1C: Use Lower Resolution (Much Smaller)**\n",
        "```bash\n",
        "# Download at lower resolution (360x181 instead of 1440x721)\n",
        "python scripts/data/download_era5.py \\\n",
        "    output_path=/path/to/your/data/era5_low_res.zarr \\\n",
        "    start_date=2019-01-01 \\\n",
        "    end_date=2019-01-31 \\\n",
        "    resolution=360x181\n",
        "```\n",
        "\n",
        "#### **Option 1D: Full Dataset (If You Have Space)**\n",
        "```bash\n",
        "# Download full dataset (several GB)\n",
        "python scripts/data/download_era5.py output_path=/path/to/your/data/era5_1993-2021-1h-1440x721.zarr\n",
        "\n",
        "# Compute statistics\n",
        "python scripts/data/data_stats.py data_path=/path/to/your/data/era5_1993-2021-1h-1440x721.zarr output_path=/path/to/your/data/stats_era5_1993-2021-1h-1440x721.zarr\n",
        "```\n",
        "\n",
        "### Option 2: Use Existing Data\n",
        "\n",
        "If you already have ERA5 data in the correct format, just update the paths in the configuration below.\n",
        "\n",
        "## Configuration\n",
        "\n",
        "Configure the reconstruction analysis parameters. Update the paths to match your model and data directories.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä **File Size Estimates**\n",
        "\n",
        "Here's how much space each option will use:\n",
        "\n",
        "| Option | Date Range | Variables | Resolution | Estimated Size | Autoencoder Compatible |\n",
        "|--------|------------|-----------|------------|----------------|----------------------|\n",
        "| **Quick Start** | 1 week | All 71 | 1440√ó721 | ~100-200 MB | ‚úÖ Yes |\n",
        "| **1A: Small Date Range** | 1 month | All 71 | 1440√ó721 | ~500 MB | ‚úÖ Yes |\n",
        "| **1B: Surface Only** | 1 month | 6 surface | 1440√ó721 | ~50 MB | ‚ùå No - Missing atmospheric |\n",
        "| **1C: Low Resolution** | 1 month | All 71 | 360√ó181 | ~30 MB | ‚úÖ Yes (but lower quality) |\n",
        "| **1D: Full Dataset** | 1993-2021 | All 71 | 1440√ó721 | ~50 GB | ‚úÖ Yes |\n",
        "\n",
        "**Recommendation**: Start with **Quick Start** (1 week, all variables) for testing, then scale up if needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üöÄ **Quick Start: Download Small Subset (FIXED)**\n",
        "\n",
        "The previous approach was downloading too much data. Here's the corrected approach:\n",
        "\n",
        "```bash\n",
        "# Navigate to the project directory first\n",
        "cd /Users/randychase/Documents/PythonWorkspace/cbottle/appa_tio\n",
        "\n",
        "# Create a data directory\n",
        "mkdir -p ~/appa_data\n",
        "\n",
        "# Use the 13-level dataset instead of 37-level (much smaller!)\n",
        "python scripts/data/download_era5.py \\\n",
        "    scripts/data/configs/download_era5.yaml \\\n",
        "    output_path=~/appa_data/era5_1week_13level.zarr \\\n",
        "    start_date=2019-01-01 \\\n",
        "    end_date=2019-01-07 \\\n",
        "    use_coarsened_levels=true\n",
        "\n",
        "# Compute statistics for this subset\n",
        "python scripts/data/data_stats.py \\\n",
        "    scripts/data/configs/data_stats.yaml \\\n",
        "    data_path=~/appa_data/era5_1week_13level.zarr \\\n",
        "    output_path=~/appa_data/stats_1week_13level.zarr \\\n",
        "    start_date=2019-01-01 \\\n",
        "    end_date=2019-01-07\n",
        "```\n",
        "\n",
        "**This should be ~50-100 MB** - much more reasonable!\n",
        "\n",
        "### üîß **Alternative: Use Lower Resolution (Even Smaller)**\n",
        "\n",
        "If you want the absolute smallest download:\n",
        "\n",
        "```bash\n",
        "# Use lower resolution (360x181 instead of 1440x721)\n",
        "python scripts/data/download_era5.py \\\n",
        "    scripts/data/configs/download_era5.yaml \\\n",
        "    output_path=~/appa_data/era5_1week_lowres.zarr \\\n",
        "    start_date=2019-01-01 \\\n",
        "    end_date=2019-01-07 \\\n",
        "    resolution=360x181 \\\n",
        "    use_coarsened_levels=true\n",
        "```\n",
        "\n",
        "**This should be ~10-20 MB** - perfect for testing!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration for reconstruction analysis\n",
        "# NOTE: You need to download ERA5 data first! See the data setup section below.\n",
        "\n",
        "# Update these paths to match your setup:\n",
        "config = {\n",
        "    'ae_model_path': '/home/azureuser/cloudfiles/code/Users/randy.chase/appa_models/autoencoders/workshop/0/',  # Update this path\n",
        "    'data_path': '/path/to/your/era5_data.zarr',  # Path to downloaded ERA5 data\n",
        "    'stats_path': '/home/azureuser/cloudfiles/code/Users/randy.chase/appa_data/data/stats_era5_1993-2021-1h-1440x721.zarr',  # Path to ERA5 statistics\n",
        "    'mask_path': '/home/azureuser/cloudfiles/code/Users/randy.chase/appa_data/data/masks_era5_1993-2021-1h-1440x721.zarr',   # Path to land/sea mask\n",
        "    'checkpoint': 'best',  # Options: 'best', 'last'\n",
        "    'start_date': '2019-01-01',\n",
        "    'end_date': '2019-01-31',  # Start with a small date range for testing\n",
        "    'batch_size': 4,\n",
        "    'num_bins': 100,  # Number of bins for error histograms\n",
        "    'sub_pressure_levels': True,  # Use sub pressure levels if available\n",
        "}\n",
        "\n",
        "# Error analysis parameters\n",
        "error_config = {\n",
        "    'bin_ranges': {\n",
        "        '2m_temperature': [-10, 10],\n",
        "        '10m_u_component_of_wind': [-10, 10],\n",
        "        '10m_v_component_of_wind': [-10, 10],\n",
        "        'mean_sea_level_pressure': [-7, 7],\n",
        "        'total_precipitation': [-4, 4],\n",
        "        'sea_surface_temperature': [-20, 20],\n",
        "        'temperature': [-10, 10],\n",
        "        'u_component_of_wind': [-10, 10],\n",
        "        'v_component_of_wind': [-10, 10],\n",
        "        'geopotential': [-600, 600],\n",
        "        'specific_humidity': [-5, 5],\n",
        "    },\n",
        "    'multipliers': {\n",
        "        'specific_humidity': 1000,  # kg/kg to g/kg\n",
        "        'total_precipitation': 1000,  # m to mm\n",
        "        'mean_sea_level_pressure': 0.01,  # Pa to hPa\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for key, value in config.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Check if model path exists\n",
        "ae_path = Path(config['ae_model_path'])\n",
        "if ae_path.exists():\n",
        "    print(f\"\\n‚úì Autoencoder path exists: {ae_path}\")\n",
        "    print(\"Contents:\")\n",
        "    for item in ae_path.iterdir():\n",
        "        print(f\"  - {item.name}\")\n",
        "else:\n",
        "    print(f\"\\n‚úó Autoencoder path does not exist: {ae_path}\")\n",
        "    print(\"Please update the 'ae_model_path' in the config above\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if data paths exist\n",
        "print(\"Checking data availability...\")\n",
        "data_checks = {\n",
        "    'ERA5 Data': config['data_path'],\n",
        "    'Statistics': config['stats_path'], \n",
        "    'Masks': config['mask_path'],\n",
        "    'Autoencoder': config['ae_model_path']\n",
        "}\n",
        "\n",
        "all_data_available = True\n",
        "for name, path in data_checks.items():\n",
        "    if Path(path).exists():\n",
        "        print(f\"‚úì {name}: {path}\")\n",
        "    else:\n",
        "        print(f\"‚úó {name}: {path} (NOT FOUND)\")\n",
        "        all_data_available = False\n",
        "\n",
        "if not all_data_available:\n",
        "    print(\"\\n‚ö†Ô∏è  Some required data is missing!\")\n",
        "    print(\"Please download the ERA5 data using the instructions above, or update the paths in the config.\")\n",
        "    print(\"You can also use the default paths from the project config:\")\n",
        "    print(f\"  data_path: {PATH_ERA5}\")\n",
        "    print(f\"  stats_path: {PATH_STAT}\")\n",
        "    print(f\"  mask_path: {PATH_MASK}\")\n",
        "else:\n",
        "    print(\"\\n‚úì All required data is available!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Autoencoder Model\n",
        "\n",
        "Load the trained autoencoder model for reconstruction analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load autoencoder model\n",
        "print(\"Loading autoencoder...\")\n",
        "ae_model = load_auto_encoder(\n",
        "    path=Path(config['ae_model_path']),\n",
        "    model_name=\"model_best\" if config['checkpoint'] == 'best' else \"model_last\",\n",
        "    device=device,\n",
        "    eval_mode=True\n",
        ")\n",
        "print(f\"Autoencoder loaded successfully\")\n",
        "\n",
        "# Get model information\n",
        "print(f\"Model latent shape: {ae_model.latent_shape}\")\n",
        "print(f\"Model device: {next(ae_model.parameters()).device}\")\n",
        "\n",
        "# Load autoencoder config to get pressure levels\n",
        "ae_config_path = Path(config['ae_model_path']) / \"config.yaml\"\n",
        "if ae_config_path.exists():\n",
        "    ae_config = compose(ae_config_path)\n",
        "    print(f\"Autoencoder config loaded\")\n",
        "    \n",
        "    # Determine pressure levels\n",
        "    if hasattr(ae_config.train, 'sub_pressure_levels') and ae_config.train.sub_pressure_levels:\n",
        "        atm_levels = SUB_PRESSURE_LEVELS\n",
        "        print(f\"Using sub pressure levels: {len(atm_levels)} levels\")\n",
        "    else:\n",
        "        atm_levels = ERA5_PRESSURE_LEVELS\n",
        "        print(f\"Using full pressure levels: {len(atm_levels)} levels\")\n",
        "else:\n",
        "    print(\"Warning: Could not load autoencoder config, using default pressure levels\")\n",
        "    atm_levels = SUB_PRESSURE_LEVELS if config['sub_pressure_levels'] else ERA5_PRESSURE_LEVELS\n",
        "\n",
        "print(f\"Pressure levels: {atm_levels}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Data Loading\n",
        "\n",
        "Set up the ERA5 dataset and data loader for reconstruction analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create standardization transform\n",
        "print(\"Setting up data standardization...\")\n",
        "st = StandardizeTransform(\n",
        "    config['stats_path'],\n",
        "    state_variables=ERA5_VARIABLES,\n",
        "    context_variables=CONTEXT_VARIABLES,\n",
        "    levels=atm_levels,\n",
        ")\n",
        "print(f\"Standardization transform created\")\n",
        "print(f\"State mean shape: {st.state_mean.shape}\")\n",
        "print(f\"State std shape: {st.state_std.shape}\")\n",
        "\n",
        "# Create dataset\n",
        "print(f\"\\nCreating ERA5 dataset...\")\n",
        "print(f\"Date range: {config['start_date']} to {config['end_date']}\")\n",
        "print(f\"Data path: {config['data_path']}\")\n",
        "\n",
        "dataset = ERA5Dataset(\n",
        "    path=config['data_path'],\n",
        "    start_date=config['start_date'],\n",
        "    end_date=config['end_date'],\n",
        "    num_samples=None,  # Use all samples in date range\n",
        "    transform=st,\n",
        "    trajectory_size=1,  # Single timestep for reconstruction\n",
        "    state_variables=ERA5_VARIABLES,\n",
        "    context_variables=CONTEXT_VARIABLES,\n",
        "    levels=atm_levels,\n",
        ")\n",
        "\n",
        "print(f\"Dataset created with {len(dataset)} samples\")\n",
        "\n",
        "# Create data loader\n",
        "dataloader = get_dataloader(\n",
        "    dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    num_workers=4,\n",
        "    prefetch_factor=2,\n",
        "    shuffle=False,  # Don't shuffle for consistent analysis\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "print(f\"Data loader created with {len(dataloader)} batches\")\n",
        "print(f\"Batch size: {config['batch_size']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Reconstruction Errors\n",
        "\n",
        "Process the ERA5 data through the autoencoder and compute various error metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize error tracking\n",
        "all_std_mse = []\n",
        "all_signed_errors = []\n",
        "all_ground_truth = []\n",
        "all_predictions = []\n",
        "all_dates = []\n",
        "\n",
        "# Setup for sea surface temperature masking if needed\n",
        "sst_idx = None\n",
        "sea_mask = None\n",
        "if \"sea_surface_temperature\" in ERA5_VARIABLES:\n",
        "    import xarray as xr\n",
        "    sst_idx = ERA5_VARIABLES.index(\"sea_surface_temperature\")\n",
        "    sea_mask_cpu = xr.open_zarr(config['mask_path'])[\"sea_surface_temperature_mask\"].values\n",
        "    sea_mask = torch.from_numpy(sea_mask_cpu).to(device)[None, None]\n",
        "    print(f\"Sea surface temperature masking enabled (index: {sst_idx})\")\n",
        "\n",
        "# Process data through autoencoder\n",
        "print(f\"\\nProcessing {len(dataloader)} batches through autoencoder...\")\n",
        "ae_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (state, context, date) in enumerate(dataloader):\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f\"Processing batch {batch_idx}/{len(dataloader)}\")\n",
        "        \n",
        "        # Move data to device\n",
        "        state = state.to(device, non_blocking=True)\n",
        "        context = context.to(device, non_blocking=True)\n",
        "        date = date.to(device, non_blocking=True)\n",
        "        \n",
        "        # Reshape for autoencoder\n",
        "        state_flat = rearrange(state, \"B T Z Lat Lon -> (B T) (Lat Lon) Z\")\n",
        "        context_flat = rearrange(context, \"B T K Lat Lon -> (B T) (Lat Lon) K\")\n",
        "        date_flat = rearrange(date, \"B T D -> (B T) D\")\n",
        "        \n",
        "        # Forward pass through autoencoder\n",
        "        _, state_pred_flat = ae_model(state_flat, date_flat, context_flat)\n",
        "        \n",
        "        # Reshape back to original format\n",
        "        state_pred = rearrange(\n",
        "            state_pred_flat, \n",
        "            \"(B T) (Lat Lon) Z -> B T Z Lat Lon\", \n",
        "            B=state.shape[0], \n",
        "            Lon=ERA5_RESOLUTION[0]\n",
        "        )\n",
        "        \n",
        "        # Compute standardized MSE (before unstandardization)\n",
        "        std_error = (state - state_pred) ** 2  # [B T Z Lat Lon]\n",
        "        std_mse = std_error.mean(dim=(-1, -2))  # [B T Z] - mean over spatial dimensions\n",
        "        \n",
        "        # Handle sea surface temperature masking\n",
        "        if sst_idx is not None:\n",
        "            # Only compute error over sea surface for SST\n",
        "            sst_error = (state[:, :, sst_idx] - state_pred[:, :, sst_idx]) ** 2\n",
        "            sst_error_masked = sst_error * sea_mask\n",
        "            std_mse[:, :, sst_idx] = sst_error_masked.mean(dim=(-1, -2))\n",
        "        \n",
        "        # Store standardized errors\n",
        "        all_std_mse.append(std_mse.cpu())\n",
        "        \n",
        "        # Unstandardize for signed error computation\n",
        "        state_unstd, _ = st.unstandardize(state.cpu())\n",
        "        state_pred_unstd, _ = st.unstandardize(state_pred.cpu())\n",
        "        \n",
        "        # Compute signed errors (in physical units)\n",
        "        signed_error = state_unstd - state_pred_unstd  # [B T Z Lat Lon]\n",
        "        \n",
        "        # Store data\n",
        "        all_signed_errors.append(signed_error)\n",
        "        all_ground_truth.append(state_unstd)\n",
        "        all_predictions.append(state_pred_unstd)\n",
        "        all_dates.append(date.cpu())\n",
        "        \n",
        "        # Clean up GPU memory\n",
        "        del state, context, date, state_pred, state_flat, context_flat, date_flat, state_pred_flat\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nReconstruction completed!\")\n",
        "print(f\"Processed {len(all_std_mse)} batches\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aggregate and Analyze Results\n",
        "\n",
        "Combine all the error data and compute summary statistics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate all results\n",
        "print(\"Aggregating results...\")\n",
        "\n",
        "# Concatenate all data\n",
        "std_mse_all = torch.cat(all_std_mse, dim=0)  # [N, T, Z]\n",
        "signed_errors_all = torch.cat(all_signed_errors, dim=0)  # [N, T, Z, Lat, Lon]\n",
        "ground_truth_all = torch.cat(all_ground_truth, dim=0)  # [N, T, Z, Lat, Lon]\n",
        "predictions_all = torch.cat(all_predictions, dim=0)  # [N, T, Z, Lat, Lon]\n",
        "dates_all = torch.cat(all_dates, dim=0)  # [N, T, D]\n",
        "\n",
        "print(f\"Aggregated data shapes:\")\n",
        "print(f\"  Standardized MSE: {std_mse_all.shape}\")\n",
        "print(f\"  Signed errors: {signed_errors_all.shape}\")\n",
        "print(f\"  Ground truth: {ground_truth_all.shape}\")\n",
        "print(f\"  Predictions: {predictions_all.shape}\")\n",
        "print(f\"  Dates: {dates_all.shape}\")\n",
        "\n",
        "# Compute summary statistics\n",
        "print(\"\\nComputing summary statistics...\")\n",
        "\n",
        "# Overall MSE statistics\n",
        "overall_mse = std_mse_all.mean(dim=(0, 1))  # Mean over samples and time\n",
        "overall_rmse = torch.sqrt(overall_mse)  # Root mean square error\n",
        "\n",
        "# Variable-wise statistics\n",
        "surface_vars = ERA5_SURFACE_VARIABLES\n",
        "atmospheric_vars = ERA5_ATMOSPHERIC_VARIABLES\n",
        "\n",
        "print(f\"\\n=== RECONSTRUCTION ERROR SUMMARY ===\")\n",
        "print(f\"Total samples: {std_mse_all.shape[0]}\")\n",
        "print(f\"Total variables: {std_mse_all.shape[2]}\")\n",
        "print(f\"Surface variables: {len(surface_vars)}\")\n",
        "print(f\"Atmospheric variables: {len(atmospheric_vars)} √ó {len(atm_levels)} = {len(atmospheric_vars) * len(atm_levels)}\")\n",
        "\n",
        "# Surface variable errors\n",
        "print(f\"\\n=== SURFACE VARIABLE ERRORS (RMSE) ===\")\n",
        "for i, var in enumerate(surface_vars):\n",
        "    rmse_val = overall_rmse[i].item()\n",
        "    print(f\"{var:25s}: {rmse_val:.4f}\")\n",
        "\n",
        "# Atmospheric variable errors (averaged across pressure levels)\n",
        "print(f\"\\n=== ATMOSPHERIC VARIABLE ERRORS (RMSE, averaged across levels) ===\")\n",
        "atm_start_idx = len(surface_vars)\n",
        "for i, var in enumerate(atmospheric_vars):\n",
        "    var_start = atm_start_idx + i * len(atm_levels)\n",
        "    var_end = var_start + len(atm_levels)\n",
        "    var_rmse = overall_rmse[var_start:var_end].mean().item()\n",
        "    print(f\"{var:25s}: {var_rmse:.4f}\")\n",
        "\n",
        "# Overall statistics\n",
        "print(f\"\\n=== OVERALL STATISTICS ===\")\n",
        "print(f\"Mean RMSE: {overall_rmse.mean().item():.4f}\")\n",
        "print(f\"Median RMSE: {overall_rmse.median().item():.4f}\")\n",
        "print(f\"Min RMSE: {overall_rmse.min().item():.4f}\")\n",
        "print(f\"Max RMSE: {overall_rmse.max().item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Error Patterns\n",
        "\n",
        "Create comprehensive visualizations of the reconstruction errors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive error visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Autoencoder Reconstruction Error Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. RMSE by variable\n",
        "ax1 = axes[0, 0]\n",
        "variable_names = surface_vars + [f\"{var}_{level}hPa\" for var in atmospheric_vars for level in atm_levels]\n",
        "rmse_values = overall_rmse.numpy()\n",
        "\n",
        "# Plot surface variables\n",
        "surface_rmse = rmse_values[:len(surface_vars)]\n",
        "x_pos = np.arange(len(surface_vars))\n",
        "bars1 = ax1.bar(x_pos, surface_rmse, alpha=0.7, label='Surface', color='skyblue')\n",
        "\n",
        "# Plot atmospheric variables (averaged across levels)\n",
        "atm_start = len(surface_vars)\n",
        "atm_rmse = []\n",
        "atm_labels = []\n",
        "for i, var in enumerate(atmospheric_vars):\n",
        "    var_start = atm_start + i * len(atm_levels)\n",
        "    var_end = var_start + len(atm_levels)\n",
        "    var_avg_rmse = rmse_values[var_start:var_end].mean()\n",
        "    atm_rmse.append(var_avg_rmse)\n",
        "    atm_labels.append(var)\n",
        "\n",
        "x_pos_atm = np.arange(len(atm_labels)) + len(surface_vars) + 1\n",
        "bars2 = ax1.bar(x_pos_atm, atm_rmse, alpha=0.7, label='Atmospheric (avg)', color='lightcoral')\n",
        "\n",
        "ax1.set_xlabel('Variables')\n",
        "ax1.set_ylabel('RMSE')\n",
        "ax1.set_title('RMSE by Variable')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Set x-axis labels\n",
        "all_labels = surface_vars + atm_labels\n",
        "all_x_pos = list(x_pos) + list(x_pos_atm)\n",
        "ax1.set_xticks(all_x_pos)\n",
        "ax1.set_xticklabels(all_labels, rotation=45, ha='right')\n",
        "\n",
        "# 2. Error distribution histogram\n",
        "ax2 = axes[0, 1]\n",
        "all_errors_flat = signed_errors_all.flatten().numpy()\n",
        "# Remove extreme outliers for better visualization\n",
        "q1, q99 = np.percentile(all_errors_flat, [1, 99])\n",
        "errors_filtered = all_errors_flat[(all_errors_flat >= q1) & (all_errors_flat <= q99)]\n",
        "\n",
        "ax2.hist(errors_filtered, bins=100, alpha=0.7, density=True, color='lightgreen')\n",
        "ax2.axvline(0, color='red', linestyle='--', alpha=0.7, label='Zero error')\n",
        "ax2.set_xlabel('Signed Error')\n",
        "ax2.set_ylabel('Density')\n",
        "ax2.set_title('Error Distribution (1st-99th percentile)')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. RMSE by pressure level (for atmospheric variables)\n",
        "ax3 = axes[1, 0]\n",
        "pressure_rmse = {}\n",
        "for i, level in enumerate(atm_levels):\n",
        "    level_rmse = []\n",
        "    for j, var in enumerate(atmospheric_vars):\n",
        "        var_idx = atm_start + j * len(atm_levels) + i\n",
        "        level_rmse.append(rmse_values[var_idx])\n",
        "    pressure_rmse[level] = np.mean(level_rmse)\n",
        "\n",
        "levels = list(pressure_rmse.keys())\n",
        "level_rmse_vals = list(pressure_rmse.values())\n",
        "ax3.plot(levels, level_rmse_vals, 'o-', linewidth=2, markersize=6)\n",
        "ax3.set_xlabel('Pressure Level (hPa)')\n",
        "ax3.set_ylabel('Average RMSE')\n",
        "ax3.set_title('RMSE vs Pressure Level')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "ax3.invert_xaxis()  # Higher pressure at bottom\n",
        "\n",
        "# 4. Spatial error pattern (example for 2m temperature)\n",
        "ax4 = axes[1, 1]\n",
        "temp_2m_idx = surface_vars.index('2m_temperature')\n",
        "temp_2m_errors = signed_errors_all[:, 0, temp_2m_idx, :, :].mean(dim=0)  # Average over samples\n",
        "\n",
        "# Create a simple spatial plot\n",
        "im = ax4.imshow(temp_2m_errors.numpy(), cmap='RdBu_r', aspect='auto')\n",
        "ax4.set_title('2m Temperature Error Pattern')\n",
        "ax4.set_xlabel('Longitude')\n",
        "ax4.set_ylabel('Latitude')\n",
        "plt.colorbar(im, ax=ax4, label='Error (K)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Error visualization completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Visualization\n",
        "\n",
        "Visualize some example reconstructions to see the quality of the autoencoder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sample reconstructions\n",
        "print(\"=== SAMPLE RECONSTRUCTION VISUALIZATION ===\")\n",
        "\n",
        "# Select a few samples for visualization\n",
        "n_samples = min(4, ground_truth_all.shape[0])\n",
        "sample_indices = torch.randperm(ground_truth_all.shape[0])[:n_samples]\n",
        "\n",
        "# Create visualization for 2m temperature\n",
        "temp_2m_idx = surface_vars.index('2m_temperature')\n",
        "\n",
        "fig, axes = plt.subplots(2, n_samples, figsize=(4*n_samples, 8))\n",
        "if n_samples == 1:\n",
        "    axes = axes.reshape(-1, 1)\n",
        "\n",
        "for i, sample_idx in enumerate(sample_indices):\n",
        "    # Ground truth\n",
        "    gt_temp = ground_truth_all[sample_idx, 0, temp_2m_idx, :, :]\n",
        "    pred_temp = predictions_all[sample_idx, 0, temp_2m_idx, :, :]\n",
        "    error_temp = signed_errors_all[sample_idx, 0, temp_2m_idx, :, :]\n",
        "    \n",
        "    # Ground truth plot\n",
        "    im1 = axes[0, i].imshow(gt_temp.numpy(), cmap='viridis', aspect='auto')\n",
        "    axes[0, i].set_title(f'Ground Truth (Sample {sample_idx})')\n",
        "    axes[0, i].set_xlabel('Longitude')\n",
        "    axes[0, i].set_ylabel('Latitude')\n",
        "    plt.colorbar(im1, ax=axes[0, i], label='Temperature (K)')\n",
        "    \n",
        "    # Prediction plot\n",
        "    im2 = axes[1, i].imshow(pred_temp.numpy(), cmap='viridis', aspect='auto')\n",
        "    axes[1, i].set_title(f'Prediction (Sample {sample_idx})')\n",
        "    axes[1, i].set_xlabel('Longitude')\n",
        "    axes[1, i].set_ylabel('Latitude')\n",
        "    plt.colorbar(im2, ax=axes[1, i], label='Temperature (K)')\n",
        "\n",
        "plt.suptitle('2m Temperature: Ground Truth vs Predictions', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Error visualization\n",
        "fig, axes = plt.subplots(1, n_samples, figsize=(4*n_samples, 4))\n",
        "if n_samples == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i, sample_idx in enumerate(sample_indices):\n",
        "    error_temp = signed_errors_all[sample_idx, 0, temp_2m_idx, :, :]\n",
        "    \n",
        "    im = axes[i].imshow(error_temp.numpy(), cmap='RdBu_r', aspect='auto')\n",
        "    axes[i].set_title(f'Error (Sample {sample_idx})')\n",
        "    axes[i].set_xlabel('Longitude')\n",
        "    axes[i].set_ylabel('Latitude')\n",
        "    plt.colorbar(im, ax=axes[i], label='Error (K)')\n",
        "\n",
        "plt.suptitle('2m Temperature Reconstruction Errors', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print sample statistics\n",
        "print(\"\\nSample Reconstruction Statistics:\")\n",
        "for i, sample_idx in enumerate(sample_indices):\n",
        "    sample_rmse = std_mse_all[sample_idx, 0, :].mean().item()\n",
        "    temp_rmse = std_mse_all[sample_idx, 0, temp_2m_idx].item()\n",
        "    print(f\"Sample {sample_idx}: Overall RMSE = {sample_rmse:.4f}, 2m Temp RMSE = {temp_rmse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results\n",
        "\n",
        "Save the analysis results for later use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save analysis results\n",
        "output_dir = Path(\"reconstruction_analysis\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"Saving analysis results...\")\n",
        "\n",
        "# Create variable names for easy reference\n",
        "variable_names = surface_vars + [f\"{var}_{level}hPa\" for var in atmospheric_vars for level in atm_levels]\n",
        "\n",
        "# Save aggregated data (smaller subset for memory efficiency)\n",
        "save_data = {\n",
        "    'std_mse': std_mse_all,\n",
        "    'overall_rmse': overall_rmse,\n",
        "    'config': config,\n",
        "    'variable_names': variable_names,\n",
        "    'surface_vars': surface_vars,\n",
        "    'atmospheric_vars': atmospheric_vars,\n",
        "    'pressure_levels': atm_levels,\n",
        "}\n",
        "\n",
        "torch.save(save_data, output_dir / \"reconstruction_analysis.pt\")\n",
        "print(f\"Analysis data saved to: {output_dir / 'reconstruction_analysis.pt'}\")\n",
        "\n",
        "# Save a few sample reconstructions for visualization\n",
        "sample_data = {\n",
        "    'ground_truth': ground_truth_all[:10],  # First 10 samples\n",
        "    'predictions': predictions_all[:10],\n",
        "    'errors': signed_errors_all[:10],\n",
        "    'dates': dates_all[:10],\n",
        "}\n",
        "\n",
        "torch.save(sample_data, output_dir / \"sample_reconstructions.pt\")\n",
        "print(f\"Sample reconstructions saved to: {output_dir / 'sample_reconstructions.pt'}\")\n",
        "\n",
        "print(f\"\\nAnalysis completed! Results saved to: {output_dir}\")\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"  - Total samples analyzed: {std_mse_all.shape[0]}\")\n",
        "print(f\"  - Date range: {config['start_date']} to {config['end_date']}\")\n",
        "print(f\"  - Overall mean RMSE: {overall_rmse.mean().item():.4f}\")\n",
        "print(f\"  - Best performing variable: {variable_names[overall_rmse.argmin().item()]} (RMSE: {overall_rmse.min().item():.4f})\")\n",
        "print(f\"  - Worst performing variable: {variable_names[overall_rmse.argmax().item()]} (RMSE: {overall_rmse.max().item():.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "This analysis provides a comprehensive view of the autoencoder's reconstruction performance. You can now:\n",
        "\n",
        "1. **Compare different models**: Run this analysis on different autoencoder checkpoints to compare performance\n",
        "2. **Temporal analysis**: Extend the date range to analyze seasonal patterns in reconstruction errors\n",
        "3. **Spatial analysis**: Investigate which geographical regions have higher reconstruction errors\n",
        "4. **Variable-specific analysis**: Deep dive into specific variables that show high errors\n",
        "5. **Error correlation**: Analyze which variables tend to have correlated reconstruction errors\n",
        "6. **Model improvement**: Use these insights to guide model architecture improvements\n",
        "\n",
        "The saved results can be loaded later for further analysis:\n",
        "```python\n",
        "results = torch.load('reconstruction_analysis/reconstruction_analysis.pt')\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
