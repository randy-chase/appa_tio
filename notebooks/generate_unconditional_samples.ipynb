{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Appa: Unconditional Sample Generation (Clean Version)\n",
        "\n",
        "This notebook demonstrates how to generate unconditional atmospheric samples using a trained Appa model.\n",
        "\n",
        "Based on the [Appa documentation](https://github.com/montefiore-sail/appa/wiki/Generating-Unconditional-Samples), this follows the blanket mechanism for generating prior trajectories.\n",
        "\n",
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from omegaconf import OmegaConf\n",
        "from einops import rearrange\n",
        "\n",
        "# Add the appa module to the path\n",
        "sys.path.append('/home/azureuser/cloudfiles/code/Users/appa_tio')\n",
        "\n",
        "import appa\n",
        "from appa.diffusion import create_denoiser, create_schedule\n",
        "from appa.sampling import DDIMSampler, PCSampler, DDPMSampler, LMSSampler, RewindDDIMSampler, select_sampler\n",
        "from appa.save import load_auto_encoder, load_denoiser\n",
        "from appa.data.datasets import LatentBlanketDataset\n",
        "from appa.date import create_trajectory_timestamps\n",
        "from appa.grid import create_icosphere\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Configure the generation parameters. Update the paths to match your downloaded model directories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration following the official wiki specifications\n",
        "config = {\n",
        "    'ae_model_path': '/home/azureuser/cloudfiles/code/Users/randy.chase/appa_models/autoencoders/workshop/0/latents/workshop/ae',  # Autoencoder path\n",
        "    'denoiser_model_path': '/home/azureuser/cloudfiles/code/Users/randy.chase/appa_models/autoencoders/workshop/0/latents/workshop/denoisers/workshop/0',  # Denoiser path\n",
        "    'model_target': 'best',  # Options: 'best', 'last'\n",
        "    'diffusion': {\n",
        "        'num_steps': 64,  # Number of denoising steps (defaults to model's validation denoising steps)\n",
        "        'sampler': {\n",
        "            'type': 'lms',  # Options: 'pc', 'ddpm', 'ddim', 'rewind', 'lms'\n",
        "            'config': {}\n",
        "        }\n",
        "    },\n",
        "    'trajectory_sizes': [72],  # Size of trajectory in hours (unpadded)\n",
        "    'num_samples_per_date': 2,  # Number of samples to generate\n",
        "    'start_dates': [\n",
        "        \"2000-04-03 0h\",\n",
        "        \"2000-04-20 12h\"\n",
        "    ],\n",
        "    'blanket_overlap': 4,  # Overlap between blankets (following wiki guidance)\n",
        "    'precision': 'float16'  # Options: 'float32', 'float16', 'bfloat16'\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for key, value in config.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Check if both model paths exist\n",
        "ae_path = config['ae_model_path']\n",
        "denoiser_path = config['denoiser_model_path']\n",
        "\n",
        "print(f\"\\nChecking model paths:\")\n",
        "print(f\"Autoencoder path: {ae_path}\")\n",
        "if os.path.exists(ae_path):\n",
        "    print(f\"✓ Autoencoder path exists\")\n",
        "    print(\"Contents:\")\n",
        "    for item in os.listdir(ae_path):\n",
        "        print(f\"  - {item}\")\n",
        "else:\n",
        "    print(f\"✗ Autoencoder path does not exist\")\n",
        "\n",
        "print(f\"\\nDenoiser path: {denoiser_path}\")\n",
        "if os.path.exists(denoiser_path):\n",
        "    print(f\"✓ Denoiser path exists\")\n",
        "    print(\"Contents:\")\n",
        "    for item in os.listdir(denoiser_path):\n",
        "        print(f\"  - {item}\")\n",
        "else:\n",
        "    print(f\"✗ Denoiser path does not exist\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Models\n",
        "\n",
        "Load the trained autoencoder and denoiser models with smart detection of model file naming conventions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Smart model loading that handles different naming conventions\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Loading autoencoder...\")\n",
        "# For Hugging Face models, use \"model\" not \"model_best\"\n",
        "ae_model = load_auto_encoder(\n",
        "    path=Path(config['ae_model_path']),\n",
        "    model_name=\"model\",  # This should be \"model\" for Hugging Face\n",
        "    device=device,\n",
        "    eval_mode=True\n",
        ")\n",
        "print(f\"Autoencoder loaded successfully\")\n",
        "\n",
        "# Get latent channels from the loaded model\n",
        "print(\"Getting latent dimensions from loaded model...\")\n",
        "latent_shape = ae_model.latent_shape\n",
        "print(f\"Model latent shape: {latent_shape}\")\n",
        "\n",
        "# Extract latent channels from the shape\n",
        "if len(latent_shape) == 3:  # ConvAE: (h, w, channels)\n",
        "    latent_channels = latent_shape[2]\n",
        "elif len(latent_shape) == 2:  # GraphAE: (nodes, channels)\n",
        "    latent_channels = latent_shape[1]\n",
        "else:\n",
        "    raise ValueError(f\"Unexpected latent shape: {latent_shape}\")\n",
        "\n",
        "print(f\"Latent channels: {latent_channels}\")\n",
        "\n",
        "print(\"Loading denoiser...\")\n",
        "# For denoiser, check if we have model_best.pth or model.pth\n",
        "denoiser_path = Path(config['denoiser_model_path'])\n",
        "\n",
        "# Check what model files exist\n",
        "model_best_path = denoiser_path / \"model_best.pth\"\n",
        "model_path = denoiser_path / \"model.pth\"\n",
        "\n",
        "if model_best_path.exists():\n",
        "    print(\"Found model_best.pth, using standard load_denoiser\")\n",
        "    best = config['model_target'] == 'best'\n",
        "    denoiser = load_denoiser(\n",
        "        path=denoiser_path,\n",
        "        best=best,\n",
        "        device=device\n",
        "    )\n",
        "elif model_path.exists():\n",
        "    print(\"Found model.pth, loading manually\")\n",
        "    # Load config\n",
        "    from omegaconf import OmegaConf\n",
        "    with open(denoiser_path / \"config.yaml\", \"r\") as f:\n",
        "        denoiser_cfg = OmegaConf.load(f)\n",
        "    \n",
        "    # Create denoiser manually\n",
        "    from appa.diffusion import create_denoiser\n",
        "    denoiser = create_denoiser(denoiser_cfg, denoiser_cfg, device=device)\n",
        "    \n",
        "    # Load weights\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    denoiser.backbone.load_state_dict(checkpoint)\n",
        "    denoiser.backbone.eval()\n",
        "else:\n",
        "    raise FileNotFoundError(f\"No model file found in {denoiser_path}\")\n",
        "\n",
        "print(f\"Denoiser loaded successfully\")\n",
        "\n",
        "# Create noise schedule\n",
        "from omegaconf import OmegaConf\n",
        "with open(denoiser_path / \"config.yaml\", \"r\") as f:\n",
        "    denoiser_cfg = OmegaConf.load(f)\n",
        "\n",
        "schedule = create_schedule(denoiser_cfg.train, device=device)\n",
        "print(f\"Noise schedule: {denoiser_cfg.train.noise_schedule}\")\n",
        "\n",
        "# Handle precision following the wiki guidance (generate.py lines 207-208, 226-227)\n",
        "precision = getattr(torch, config['precision'])\n",
        "use_bfloat16 = precision == torch.bfloat16\n",
        "\n",
        "if use_bfloat16:\n",
        "    torch.set_default_dtype(torch.bfloat16)\n",
        "    print(f\"Set default dtype to {torch.get_default_dtype()}\")\n",
        "\n",
        "print(f\"Using precision: {config['precision']}\")\n",
        "print(f\"Use bfloat16: {use_bfloat16}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Sampling\n",
        "\n",
        "Configure the sampler based on the configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sampler using the select_sampler function\n",
        "sampler_type = config['diffusion']['sampler']['type']\n",
        "sampler_config = config['diffusion']['sampler']['config']\n",
        "num_steps = config['diffusion']['num_steps']\n",
        "\n",
        "# Use the select_sampler function to get the correct sampler class\n",
        "SamplerClass = select_sampler(sampler_type)\n",
        "\n",
        "# Create the sampler instance\n",
        "sampler = SamplerClass(\n",
        "    denoiser=denoiser,\n",
        "    schedule=schedule,\n",
        "    steps=num_steps,\n",
        "    **sampler_config\n",
        ")\n",
        "\n",
        "print(f\"Sampler created: {sampler_type}\")\n",
        "print(f\"Number of steps: {num_steps}\")\n",
        "print(f\"Sampler class: {SamplerClass.__name__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Unconditional Samples\n",
        "\n",
        "Generate unconditional atmospheric samples for the specified dates and trajectory sizes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse start dates\n",
        "start_dates = []\n",
        "for date_str in config['start_dates']:\n",
        "    # Parse date string like \"2000-04-03 0h\"\n",
        "    date_part, hour_part = date_str.split()\n",
        "    hour = int(hour_part.replace('h', ''))\n",
        "    start_dates.append((date_part, hour))\n",
        "\n",
        "print(f\"Start dates: {start_dates}\")\n",
        "print(f\"Trajectory sizes: {config['trajectory_sizes']}\")\n",
        "print(f\"Samples per date: {config['num_samples_per_date']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate samples for each trajectory size and start date\n",
        "# Following the official generate.py pattern with proper trajectory padding\n",
        "all_samples = {}\n",
        "\n",
        "for unpadded_trajectory_size in config['trajectory_sizes']:\n",
        "    print(f\"\\nGenerating samples for trajectory size: {unpadded_trajectory_size}h\")\n",
        "    \n",
        "    # CRITICAL: Implement trajectory padding logic from generate.py lines 64-68\n",
        "    # This ensures we have a valid number of blankets\n",
        "    blanket_size = denoiser_cfg.train.blanket_size\n",
        "    blanket_stride = blanket_size - config['blanket_overlap']\n",
        "    \n",
        "    # Pad trajectory to fit blankets properly (following generate.py logic)\n",
        "    padded_trajectory_size = max(blanket_size, unpadded_trajectory_size)\n",
        "    while (padded_trajectory_size - blanket_size) % blanket_stride != 0:\n",
        "        padded_trajectory_size += 1\n",
        "    \n",
        "    print(f\"  Unpadded size: {unpadded_trajectory_size}h\")\n",
        "    print(f\"  Padded size: {padded_trajectory_size}h\")\n",
        "    print(f\"  Blanket size: {blanket_size}, stride: {blanket_stride}\")\n",
        "    \n",
        "    trajectory_samples = {}\n",
        "    \n",
        "    for date_str, hour in start_dates:\n",
        "        print(f\"  Date: {date_str} {hour:02d}h\")\n",
        "        \n",
        "        # Create timestamps for the PADDED trajectory (following generate.py line 247-249)\n",
        "        timestamps = create_trajectory_timestamps(\n",
        "            start_day=date_str,\n",
        "            start_hour=hour,\n",
        "            traj_size=padded_trajectory_size,\n",
        "            dt=1  # 1 hour timestep\n",
        "        )[None]  # Add batch dimension like in generate.py\n",
        "        \n",
        "        # Generate multiple samples for this date\n",
        "        date_samples = []\n",
        "        \n",
        "        for sample_idx in range(config['num_samples_per_date']):\n",
        "            print(f\"    Sample {sample_idx + 1}/{config['num_samples_per_date']}\")\n",
        "            \n",
        "            # Generate sample following the EXACT Appa generate.py pattern\n",
        "            # Handle precision as in generate.py lines 284-288\n",
        "            def _generate_sample():\n",
        "                from appa.diffusion import TrajectoryDenoiser, Denoiser\n",
        "                from functools import partial\n",
        "                import math\n",
        "                \n",
        "                # Get latent shape and state size (following generate.py lines 221-224)\n",
        "                latent_shape = ae_model.latent_shape\n",
        "                if len(latent_shape) == 3:\n",
        "                    latent_shape = latent_shape[0] * latent_shape[1], latent_shape[2]\n",
        "                state_size = math.prod(latent_shape)\n",
        "                \n",
        "                # Create Denoiser wrapper first (following generate.py lines 251-253)\n",
        "                denoise = Denoiser(denoiser.backbone).cuda()\n",
        "                \n",
        "                # Handle precision properly (following generate.py lines 252-253)\n",
        "                if use_bfloat16:\n",
        "                    denoise = denoise.to(torch.bfloat16)\n",
        "                \n",
        "                # Create TrajectoryDenoiser (following generate.py lines 255-262)\n",
        "                trajectory_denoiser = TrajectoryDenoiser(\n",
        "                    denoise,\n",
        "                    blanket_size=blanket_size,\n",
        "                    blanket_stride=blanket_stride,\n",
        "                    state_size=state_size,\n",
        "                    distributed=False,  # Single GPU for notebook (vs True in distributed version)\n",
        "                    pass_blanket_ids=False,\n",
        "                )\n",
        "                \n",
        "                # Bind date parameter using partial (following generate.py line 264)\n",
        "                conditioned_denoiser = partial(trajectory_denoiser, date=timestamps.cuda())\n",
        "                \n",
        "                # Create sampler with conditioned denoiser (following generate.py lines 268-276)\n",
        "                sampler = SamplerClass(\n",
        "                    denoiser=conditioned_denoiser,\n",
        "                    schedule=schedule,\n",
        "                    steps=num_steps,\n",
        "                    silent=False,\n",
        "                    **sampler_config\n",
        "                )\n",
        "                \n",
        "                # Generate random noise and scale by max sigma (following generate.py lines 277-279)\n",
        "                x1 = torch.randn(len(timestamps), padded_trajectory_size * state_size, device=device)\n",
        "                samp_start = (x1 * schedule.sigma_tmax().cuda()).flatten(1).cuda()\n",
        "                \n",
        "                # Sample using the sampler (following generate.py line 279)\n",
        "                sample = sampler(samp_start).reshape((-1, padded_trajectory_size, *latent_shape))\n",
        "                \n",
        "                # CRITICAL: Trim to unpadded trajectory size (following generate.py line 292)\n",
        "                sample = sample[:, :unpadded_trajectory_size]\n",
        "                \n",
        "                return sample\n",
        "            \n",
        "            # Handle precision as in generate.py lines 284-288\n",
        "            if precision != torch.float16:\n",
        "                sample = _generate_sample()\n",
        "            else:\n",
        "                with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "                    sample = _generate_sample()\n",
        "            \n",
        "            date_samples.append(sample.cpu())\n",
        "        \n",
        "        trajectory_samples[f\"{date_str}_{hour:02d}h\"] = torch.cat(date_samples, dim=0)\n",
        "    \n",
        "    all_samples[f\"{unpadded_trajectory_size}h\"] = trajectory_samples\n",
        "\n",
        "print(\"\\nGeneration completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decode Samples to Physical Space\n",
        "\n",
        "Decode the generated latent samples back to physical atmospheric variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decode samples to physical space\n",
        "decoded_samples = {}\n",
        "\n",
        "for trajectory_size, trajectory_data in all_samples.items():\n",
        "    print(f\"Decoding samples for {trajectory_size}...\")\n",
        "    \n",
        "    decoded_trajectory = {}\n",
        "    \n",
        "    for date_key, samples in trajectory_data.items():\n",
        "        print(f\"  Decoding {date_key}...\")\n",
        "        \n",
        "        # Move samples to device for decoding\n",
        "        samples = samples.to(device)\n",
        "        \n",
        "        # Decode each sample\n",
        "        decoded_batch = []\n",
        "        for i in range(samples.shape[0]):\n",
        "            with torch.no_grad():\n",
        "                # Decode the latent sample to physical space\n",
        "                decoded = ae_model.decode(samples[i:i+1])\n",
        "                decoded_batch.append(decoded.cpu())\n",
        "        \n",
        "        decoded_trajectory[date_key] = torch.cat(decoded_batch, dim=0)\n",
        "    \n",
        "    decoded_samples[trajectory_size] = decoded_trajectory\n",
        "\n",
        "print(\"Decoding completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results\n",
        "\n",
        "Save the generated samples for later analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "output_dir = Path(\"generated_samples\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save latent samples\n",
        "latent_path = output_dir / \"latent_samples.pt\"\n",
        "torch.save(all_samples, latent_path)\n",
        "print(f\"Latent samples saved to: {latent_path}\")\n",
        "\n",
        "# Save decoded samples\n",
        "decoded_path = output_dir / \"decoded_samples.pt\"\n",
        "torch.save(decoded_samples, decoded_path)\n",
        "print(f\"Decoded samples saved to: {decoded_path}\")\n",
        "\n",
        "# Save configuration\n",
        "config_path = output_dir / \"generation_config.yaml\"\n",
        "OmegaConf.save(config, config_path)\n",
        "print(f\"Configuration saved to: {config_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization\n",
        "\n",
        "Create some basic visualizations of the generated samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic visualization of sample statistics\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, (trajectory_size, trajectory_data) in enumerate(decoded_samples.items()):\n",
        "    if idx >= 4:\n",
        "        break\n",
        "    \n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # Get all samples for this trajectory size\n",
        "    all_trajectory_samples = torch.cat(list(trajectory_data.values()), dim=0)\n",
        "    \n",
        "    # Plot mean and std across samples\n",
        "    mean_values = all_trajectory_samples.mean(dim=(0, 1))  # Mean across batch and time\n",
        "    std_values = all_trajectory_samples.std(dim=(0, 1))   # Std across batch and time\n",
        "    \n",
        "    ax.plot(mean_values.numpy(), label='Mean', alpha=0.7)\n",
        "    ax.fill_between(range(len(mean_values)), \n",
        "                    (mean_values - std_values).numpy(), \n",
        "                    (mean_values + std_values).numpy(), \n",
        "                    alpha=0.3, label='±1σ')\n",
        "    \n",
        "    ax.set_title(f'Trajectory {trajectory_size} - Sample Statistics')\n",
        "    ax.set_xlabel('Variable Index')\n",
        "    ax.set_ylabel('Value')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print sample information\n",
        "print(\"\\nGenerated Sample Summary:\")\n",
        "for trajectory_size, trajectory_data in decoded_samples.items():\n",
        "    print(f\"\\nTrajectory {trajectory_size}:\")\n",
        "    for date_key, samples in trajectory_data.items():\n",
        "        print(f\"  {date_key}: {samples.shape} samples\")\n",
        "        print(f\"    Mean: {samples.mean().item():.4f}\")\n",
        "        print(f\"    Std: {samples.std().item():.4f}\")\n",
        "        print(f\"    Min: {samples.min().item():.4f}\")\n",
        "        print(f\"    Max: {samples.max().item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "The generated samples can now be used for:\n",
        "\n",
        "1. **Rendering**: Use the rendering scripts to visualize the atmospheric states\n",
        "2. **Evaluation**: Compare against ground truth data for validation\n",
        "3. **Conditional Generation**: Use these as starting points for data assimilation\n",
        "4. **Forecasting**: Use as initial conditions for weather forecasting\n",
        "\n",
        "For more advanced usage, see the [Appa documentation](https://github.com/montefiore-sail/appa/wiki) for:\n",
        "- [Forecasting](https://github.com/montefiore-sail/appa/wiki/Forecasting)\n",
        "- [Reanalysis and Filtering](https://github.com/montefiore-sail/appa/wiki/Reanalysis-and-Filtering)\n",
        "- [Rendering States](https://github.com/montefiore-sail/appa/wiki/Rendering-States)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
