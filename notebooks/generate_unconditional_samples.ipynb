{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Appa: Unconditional Sample Generation (Clean Version)\n",
        "\n",
        "This notebook demonstrates how to generate unconditional atmospheric samples using a trained Appa model.\n",
        "\n",
        "Based on the [Appa documentation](https://github.com/montefiore-sail/appa/wiki/Generating-Unconditional-Samples), this follows the blanket mechanism for generating prior trajectories.\n",
        "\n",
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from omegaconf import OmegaConf\n",
        "from einops import rearrange\n",
        "\n",
        "# Add the appa module to the path\n",
        "sys.path.append('/home/azureuser/cloudfiles/code/Users/appa_tio')\n",
        "\n",
        "import appa\n",
        "from appa.diffusion import create_denoiser, create_schedule\n",
        "from appa.sampling import DDIMSampler, PCSampler, DDPMSampler, LMSSampler, RewindDDIMSampler, select_sampler\n",
        "from appa.save import load_auto_encoder, load_denoiser\n",
        "from appa.data.datasets import LatentBlanketDataset\n",
        "from appa.date import create_trajectory_timestamps\n",
        "from appa.grid import create_icosphere\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Configure the generation parameters. Update the paths to match your downloaded model directories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration following the official wiki specifications\n",
        "config = {\n",
        "    'ae_model_path': '/home/azureuser/cloudfiles/code/Users/randy.chase/appa_models/autoencoders/workshop/0/latents/workshop/ae',  # Autoencoder path\n",
        "    'denoiser_model_path': '/home/azureuser/cloudfiles/code/Users/randy.chase/appa_models/autoencoders/workshop/0/latents/workshop/denoisers/workshop/0',  # Denoiser path\n",
        "    'model_target': 'best',  # Options: 'best', 'last'\n",
        "    'diffusion': {\n",
        "        'num_steps': 64,  # Number of denoising steps (defaults to model's validation denoising steps)\n",
        "        'sampler': {\n",
        "            'type': 'lms',  # Options: 'pc', 'ddpm', 'ddim', 'rewind', 'lms'\n",
        "            'config': {}\n",
        "        }\n",
        "    },\n",
        "    'trajectory_sizes': [72],  # Size of trajectory in hours (unpadded)\n",
        "    'num_samples_per_date': 2,  # Number of samples to generate\n",
        "    'start_dates': [\n",
        "        \"2000-04-03 0h\",\n",
        "        \"2000-04-20 12h\"\n",
        "    ],\n",
        "    'blanket_overlap': 4,  # Overlap between blankets (following wiki guidance)\n",
        "    'precision': 'float16'  # Options: 'float32', 'float16', 'bfloat16'\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for key, value in config.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Check if both model paths exist\n",
        "ae_path = config['ae_model_path']\n",
        "denoiser_path = config['denoiser_model_path']\n",
        "\n",
        "print(f\"\\nChecking model paths:\")\n",
        "print(f\"Autoencoder path: {ae_path}\")\n",
        "if os.path.exists(ae_path):\n",
        "    print(f\"✓ Autoencoder path exists\")\n",
        "    print(\"Contents:\")\n",
        "    for item in os.listdir(ae_path):\n",
        "        print(f\"  - {item}\")\n",
        "else:\n",
        "    print(f\"✗ Autoencoder path does not exist\")\n",
        "\n",
        "print(f\"\\nDenoiser path: {denoiser_path}\")\n",
        "if os.path.exists(denoiser_path):\n",
        "    print(f\"✓ Denoiser path exists\")\n",
        "    print(\"Contents:\")\n",
        "    for item in os.listdir(denoiser_path):\n",
        "        print(f\"  - {item}\")\n",
        "else:\n",
        "    print(f\"✗ Denoiser path does not exist\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Models\n",
        "\n",
        "Load the trained autoencoder and denoiser models with smart detection of model file naming conventions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Smart model loading that handles different naming conventions\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Loading autoencoder...\")\n",
        "# For Hugging Face models, use \"model\" not \"model_best\"\n",
        "ae_model = load_auto_encoder(\n",
        "    path=Path(config['ae_model_path']),\n",
        "    model_name=\"model\",  # This should be \"model\" for Hugging Face\n",
        "    device=device,\n",
        "    eval_mode=True\n",
        ")\n",
        "print(f\"Autoencoder loaded successfully\")\n",
        "\n",
        "# Get latent channels from the loaded model\n",
        "print(\"Getting latent dimensions from loaded model...\")\n",
        "latent_shape = ae_model.latent_shape\n",
        "print(f\"Model latent shape: {latent_shape}\")\n",
        "\n",
        "# Extract latent channels from the shape\n",
        "if len(latent_shape) == 3:  # ConvAE: (h, w, channels)\n",
        "    latent_channels = latent_shape[2]\n",
        "elif len(latent_shape) == 2:  # GraphAE: (nodes, channels)\n",
        "    latent_channels = latent_shape[1]\n",
        "else:\n",
        "    raise ValueError(f\"Unexpected latent shape: {latent_shape}\")\n",
        "\n",
        "print(f\"Latent channels: {latent_channels}\")\n",
        "\n",
        "print(\"Loading denoiser...\")\n",
        "# For denoiser, check if we have model_best.pth or model.pth\n",
        "denoiser_path = Path(config['denoiser_model_path'])\n",
        "\n",
        "# Check what model files exist\n",
        "model_best_path = denoiser_path / \"model_best.pth\"\n",
        "model_path = denoiser_path / \"model.pth\"\n",
        "\n",
        "if model_best_path.exists():\n",
        "    print(\"Found model_best.pth, using standard load_denoiser\")\n",
        "    best = config['model_target'] == 'best'\n",
        "    denoiser = load_denoiser(\n",
        "        path=denoiser_path,\n",
        "        best=best,\n",
        "        device=device\n",
        "    )\n",
        "elif model_path.exists():\n",
        "    print(\"Found model.pth, loading manually\")\n",
        "    # Load config\n",
        "    from omegaconf import OmegaConf\n",
        "    with open(denoiser_path / \"config.yaml\", \"r\") as f:\n",
        "        denoiser_cfg = OmegaConf.load(f)\n",
        "    \n",
        "    # Create denoiser manually\n",
        "    from appa.diffusion import create_denoiser\n",
        "    denoiser = create_denoiser(denoiser_cfg, denoiser_cfg, device=device)\n",
        "    \n",
        "    # Load weights\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    denoiser.backbone.load_state_dict(checkpoint)\n",
        "    denoiser.backbone.eval()\n",
        "else:\n",
        "    raise FileNotFoundError(f\"No model file found in {denoiser_path}\")\n",
        "\n",
        "print(f\"Denoiser loaded successfully\")\n",
        "\n",
        "# Create noise schedule\n",
        "from omegaconf import OmegaConf\n",
        "with open(denoiser_path / \"config.yaml\", \"r\") as f:\n",
        "    denoiser_cfg = OmegaConf.load(f)\n",
        "\n",
        "schedule = create_schedule(denoiser_cfg.train, device=device)\n",
        "print(f\"Noise schedule: {denoiser_cfg.train.noise_schedule}\")\n",
        "\n",
        "# Handle precision following the wiki guidance (generate.py lines 207-208, 226-227)\n",
        "precision = getattr(torch, config['precision'])\n",
        "use_bfloat16 = precision == torch.bfloat16\n",
        "\n",
        "if use_bfloat16:\n",
        "    torch.set_default_dtype(torch.bfloat16)\n",
        "    print(f\"Set default dtype to {torch.get_default_dtype()}\")\n",
        "\n",
        "print(f\"Using precision: {config['precision']}\")\n",
        "print(f\"Use bfloat16: {use_bfloat16}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Sampling\n",
        "\n",
        "Configure the sampler based on the configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sampler using the select_sampler function\n",
        "sampler_type = config['diffusion']['sampler']['type']\n",
        "sampler_config = config['diffusion']['sampler']['config']\n",
        "num_steps = config['diffusion']['num_steps']\n",
        "\n",
        "# Use the select_sampler function to get the correct sampler class\n",
        "SamplerClass = select_sampler(sampler_type)\n",
        "\n",
        "# Create the sampler instance\n",
        "sampler = SamplerClass(\n",
        "    denoiser=denoiser,\n",
        "    schedule=schedule,\n",
        "    steps=num_steps,\n",
        "    **sampler_config\n",
        ")\n",
        "\n",
        "print(f\"Sampler created: {sampler_type}\")\n",
        "print(f\"Number of steps: {num_steps}\")\n",
        "print(f\"Sampler class: {SamplerClass.__name__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Unconditional Samples\n",
        "\n",
        "Generate unconditional atmospheric samples for the specified dates and trajectory sizes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse start dates\n",
        "start_dates = []\n",
        "for date_str in config['start_dates']:\n",
        "    # Parse date string like \"2000-04-03 0h\"\n",
        "    date_part, hour_part = date_str.split()\n",
        "    hour = int(hour_part.replace('h', ''))\n",
        "    start_dates.append((date_part, hour))\n",
        "\n",
        "print(f\"Start dates: {start_dates}\")\n",
        "print(f\"Trajectory sizes: {config['trajectory_sizes']}\")\n",
        "print(f\"Samples per date: {config['num_samples_per_date']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate samples for each trajectory size and start date\n",
        "# Following the official generate.py pattern with proper trajectory padding\n",
        "all_samples = {}\n",
        "\n",
        "for unpadded_trajectory_size in config['trajectory_sizes']:\n",
        "    print(f\"\\nGenerating samples for trajectory size: {unpadded_trajectory_size}h\")\n",
        "    \n",
        "    # CRITICAL: Implement trajectory padding logic from generate.py lines 64-68\n",
        "    # This ensures we have a valid number of blankets\n",
        "    blanket_size = denoiser_cfg.train.blanket_size\n",
        "    blanket_stride = blanket_size - config['blanket_overlap']\n",
        "    \n",
        "    # Pad trajectory to fit blankets properly (following generate.py logic)\n",
        "    padded_trajectory_size = max(blanket_size, unpadded_trajectory_size)\n",
        "    while (padded_trajectory_size - blanket_size) % blanket_stride != 0:\n",
        "        padded_trajectory_size += 1\n",
        "    \n",
        "    print(f\"  Unpadded size: {unpadded_trajectory_size}h\")\n",
        "    print(f\"  Padded size: {padded_trajectory_size}h\")\n",
        "    print(f\"  Blanket size: {blanket_size}, stride: {blanket_stride}\")\n",
        "    \n",
        "    trajectory_samples = {}\n",
        "    \n",
        "    for date_str, hour in start_dates:\n",
        "        print(f\"  Date: {date_str} {hour:02d}h\")\n",
        "        \n",
        "        # Create timestamps for the PADDED trajectory (following generate.py line 247-249)\n",
        "        timestamps = create_trajectory_timestamps(\n",
        "            start_day=date_str,\n",
        "            start_hour=hour,\n",
        "            traj_size=padded_trajectory_size,\n",
        "            dt=1  # 1 hour timestep\n",
        "        )[None]  # Add batch dimension like in generate.py\n",
        "        \n",
        "        # Generate multiple samples for this date\n",
        "        date_samples = []\n",
        "        \n",
        "        for sample_idx in range(config['num_samples_per_date']):\n",
        "            print(f\"    Sample {sample_idx + 1}/{config['num_samples_per_date']}\")\n",
        "            \n",
        "            # Generate sample following the EXACT Appa generate.py pattern\n",
        "            # Handle precision as in generate.py lines 284-288\n",
        "            def _generate_sample():\n",
        "                from appa.diffusion import TrajectoryDenoiser, Denoiser\n",
        "                from functools import partial\n",
        "                import math\n",
        "                \n",
        "                # Get latent shape and state size (following generate.py lines 221-224)\n",
        "                latent_shape = ae_model.latent_shape\n",
        "                if len(latent_shape) == 3:\n",
        "                    latent_shape = latent_shape[0] * latent_shape[1], latent_shape[2]\n",
        "                state_size = math.prod(latent_shape)\n",
        "                \n",
        "                # Create Denoiser wrapper first (following generate.py lines 251-253)\n",
        "                denoise = Denoiser(denoiser.backbone).cuda()\n",
        "                \n",
        "                # Handle precision properly (following generate.py lines 252-253)\n",
        "                if use_bfloat16:\n",
        "                    denoise = denoise.to(torch.bfloat16)\n",
        "                \n",
        "                # Create TrajectoryDenoiser (following generate.py lines 255-262)\n",
        "                trajectory_denoiser = TrajectoryDenoiser(\n",
        "                    denoise,\n",
        "                    blanket_size=blanket_size,\n",
        "                    blanket_stride=blanket_stride,\n",
        "                    state_size=state_size,\n",
        "                    distributed=False,  # Single GPU for notebook (vs True in distributed version)\n",
        "                    pass_blanket_ids=False,\n",
        "                )\n",
        "                \n",
        "                # Bind date parameter using partial (following generate.py line 264)\n",
        "                conditioned_denoiser = partial(trajectory_denoiser, date=timestamps.cuda())\n",
        "                \n",
        "                # Create sampler with conditioned denoiser (following generate.py lines 268-276)\n",
        "                sampler = SamplerClass(\n",
        "                    denoiser=conditioned_denoiser,\n",
        "                    schedule=schedule,\n",
        "                    steps=num_steps,\n",
        "                    silent=False,\n",
        "                    **sampler_config\n",
        "                )\n",
        "                \n",
        "                # Generate random noise and scale by max sigma (following generate.py lines 277-279)\n",
        "                x1 = torch.randn(len(timestamps), padded_trajectory_size * state_size, device=device)\n",
        "                samp_start = (x1 * schedule.sigma_tmax().cuda()).flatten(1).cuda()\n",
        "                \n",
        "                # Sample using the sampler (following generate.py line 279)\n",
        "                sample = sampler(samp_start).reshape((-1, padded_trajectory_size, *latent_shape))\n",
        "                \n",
        "                # CRITICAL: Trim to unpadded trajectory size (following generate.py line 292)\n",
        "                sample = sample[:, :unpadded_trajectory_size]\n",
        "                \n",
        "                return sample\n",
        "            \n",
        "            # Handle precision as in generate.py lines 284-288\n",
        "            if precision != torch.float16:\n",
        "                sample = _generate_sample()\n",
        "            else:\n",
        "                with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "                    sample = _generate_sample()\n",
        "            \n",
        "            date_samples.append(sample.cpu())\n",
        "        \n",
        "        trajectory_samples[f\"{date_str}_{hour:02d}h\"] = torch.cat(date_samples, dim=0)\n",
        "    \n",
        "    all_samples[f\"{unpadded_trajectory_size}h\"] = trajectory_samples\n",
        "\n",
        "print(\"\\nGeneration completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decode Samples to Physical Space\n",
        "\n",
        "Decode the generated latent samples back to physical atmospheric variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decode samples to physical space\n",
        "# Following the original generate.py pattern with proper timestamp handling\n",
        "decoded_samples = {}\n",
        "\n",
        "# CRITICAL: Clean up memory before decoding (following generate.py pattern)\n",
        "# Move denoiser to CPU and clear GPU cache to free memory for decoding\n",
        "print(\"Cleaning up GPU memory before decoding...\")\n",
        "if 'denoiser' in locals():\n",
        "    denoiser = denoiser.cpu()\n",
        "    del denoiser\n",
        "if 'schedule' in locals():\n",
        "    schedule = schedule.cpu()\n",
        "    del schedule\n",
        "if 'denoiser_cfg' in locals():\n",
        "    del denoiser_cfg\n",
        "\n",
        "# Clear GPU cache\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "# Move autoencoder to GPU for decoding (following generate.py pattern)\n",
        "ae_model = ae_model.to(device)\n",
        "print(f\"Autoencoder moved to device: {device}\")\n",
        "print(f\"GPU memory after cleanup: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "\n",
        "for trajectory_size, trajectory_data in all_samples.items():\n",
        "    print(f\"Decoding samples for {trajectory_size}...\")\n",
        "    \n",
        "    decoded_trajectory = {}\n",
        "    \n",
        "    for date_key, samples in trajectory_data.items():\n",
        "        print(f\"  Decoding {date_key}...\")\n",
        "        \n",
        "        # Parse date and hour from the key\n",
        "        date_str, hour_str = date_key.split('_')\n",
        "        hour = int(hour_str.replace('h', ''))\n",
        "        \n",
        "        # Create timestamps for this trajectory (following generate.py pattern)\n",
        "        timestamps = create_trajectory_timestamps(\n",
        "            start_day=date_str,\n",
        "            start_hour=hour,\n",
        "            traj_size=int(trajectory_size.replace('h', '')),\n",
        "            dt=1  # 1 hour timestep\n",
        "        )\n",
        "        \n",
        "        # Move samples to device for decoding\n",
        "        samples = samples.to(device)\n",
        "        timestamps = timestamps.to(device)\n",
        "        \n",
        "        # Decode each sample with proper timestamps\n",
        "        # CRITICAL: Decode one timestep at a time to avoid massive memory usage\n",
        "        decoded_batch = []\n",
        "        for i in range(samples.shape[0]):\n",
        "            print(f\"    Decoding sample {i+1}/{samples.shape[0]}...\")\n",
        "            \n",
        "            # Decode each timestep individually (following rendering script pattern)\n",
        "            sample_decoded = []\n",
        "            for t in range(samples.shape[1]):  # For each timestep in the trajectory\n",
        "                with torch.no_grad():\n",
        "                    # Decode single timestep: z (1, latent_channels), t (1, 4), c (optional)\n",
        "                    z_timestep = samples[i, t:t+1]  # Shape: (1, latent_channels)\n",
        "                    t_timestep = timestamps[t:t+1]  # Shape: (1, 4)\n",
        "                    \n",
        "                    # Decode single timestep (much more memory efficient)\n",
        "                    decoded_timestep = ae_model.decode(z_timestep, t_timestep)\n",
        "                    \n",
        "                    # Rearrange from (T, Lat×Lon, Z) to (T, Z, Lat, Lon) following render.py pattern\n",
        "                    from einops import rearrange\n",
        "                    decoded_timestep = rearrange(\n",
        "                        decoded_timestep, \n",
        "                        \"T (Lat Lon) Z -> T Z Lat Lon\", \n",
        "                        Lat=721  # ERA5_RESOLUTION[1]\n",
        "                    )\n",
        "                    \n",
        "                    sample_decoded.append(decoded_timestep.cpu())\n",
        "            \n",
        "            # Stack all timesteps for this sample\n",
        "            sample_decoded = torch.cat(sample_decoded, dim=0)  # Shape: (trajectory_length, ...)\n",
        "            decoded_batch.append(sample_decoded)\n",
        "            \n",
        "            # Clean up memory after each sample\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        decoded_trajectory[date_key] = torch.cat(decoded_batch, dim=0)\n",
        "        \n",
        "        # Clean up memory after each trajectory\n",
        "        del samples, timestamps, decoded_batch\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "    \n",
        "    decoded_samples[trajectory_size] = decoded_trajectory\n",
        "\n",
        "print(\"Decoding completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results\n",
        "\n",
        "Save the generated samples for later analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "output_dir = Path(\"generated_samples\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save latent samples\n",
        "latent_path = output_dir / \"latent_samples.pt\"\n",
        "torch.save(all_samples, latent_path)\n",
        "print(f\"Latent samples saved to: {latent_path}\")\n",
        "\n",
        "# Save decoded samples\n",
        "decoded_path = output_dir / \"decoded_samples.pt\"\n",
        "torch.save(decoded_samples, decoded_path)\n",
        "print(f\"Decoded samples saved to: {decoded_path}\")\n",
        "\n",
        "# Save configuration\n",
        "config_path = output_dir / \"generation_config.yaml\"\n",
        "OmegaConf.save(config, config_path)\n",
        "print(f\"Configuration saved to: {config_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization\n",
        "\n",
        "Create some basic visualizations of the generated samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Variable Names and Channel Mapping\n",
        "# Let's create a mapping of the 71 atmospheric variables to understand what each channel represents\n",
        "\n",
        "# Import the constants\n",
        "from appa.data.const import ERA5_SURFACE_VARIABLES, ERA5_ATMOSPHERIC_VARIABLES, SUB_PRESSURE_LEVELS\n",
        "\n",
        "# Create the complete variable list\n",
        "surface_vars = ERA5_SURFACE_VARIABLES  # 6 variables\n",
        "atmospheric_vars = ERA5_ATMOSPHERIC_VARIABLES  # 5 variables\n",
        "pressure_levels = SUB_PRESSURE_LEVELS  # 13 levels\n",
        "\n",
        "# Build the complete variable list\n",
        "all_variables = []\n",
        "channel_idx = 0\n",
        "\n",
        "print(\"=== SURFACE VARIABLES (6 channels) ===\")\n",
        "# Add surface variables (6 channels)\n",
        "for var in surface_vars:\n",
        "    all_variables.append(f\"{var} (surface)\")\n",
        "    print(f\"Channel {channel_idx}: {var} (surface)\")\n",
        "    channel_idx += 1\n",
        "\n",
        "print(\"\\n=== ATMOSPHERIC VARIABLES (65 channels) ===\")\n",
        "# Add atmospheric variables at each pressure level (5 × 13 = 65 channels)\n",
        "for var in atmospheric_vars:\n",
        "    for level in pressure_levels:\n",
        "        all_variables.append(f\"{var} at {level}hPa\")\n",
        "        print(f\"Channel {channel_idx}: {var} at {level}hPa\")\n",
        "        channel_idx += 1\n",
        "\n",
        "print(f\"\\n=== SUMMARY ===\")\n",
        "print(f\"Total variables: {len(all_variables)}\")\n",
        "print(f\"Surface variables: {len(surface_vars)}\")\n",
        "print(f\"Atmospheric variables: {len(atmospheric_vars)} × {len(pressure_levels)} = {len(atmospheric_vars) * len(pressure_levels)}\")\n",
        "\n",
        "# Store the variable names for later use\n",
        "variable_names = all_variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unscale Decoded Samples to Original Physical Units\n",
        "# Following the pattern from the rendering scripts\n",
        "\n",
        "# Install huggingface_hub if not available\n",
        "try:\n",
        "    import huggingface_hub\n",
        "except ImportError:\n",
        "    print(\"Installing huggingface_hub...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"huggingface_hub\"])\n",
        "    import huggingface_hub\n",
        "\n",
        "def download_appa_data_files():\n",
        "    \"\"\"\n",
        "    Download the required ERA5 statistics files from Hugging Face.\n",
        "    \"\"\"\n",
        "    from huggingface_hub import snapshot_download\n",
        "    from pathlib import Path\n",
        "    \n",
        "    print(\"Downloading ERA5 statistics files from Hugging Face...\")\n",
        "    \n",
        "    # Download the data files from the Hugging Face dataset\n",
        "    data_dir = snapshot_download(\n",
        "        repo_id=\"montefiore-sail/appa\",\n",
        "        repo_type=\"dataset\",\n",
        "        allow_patterns=[\"data/*\"],\n",
        "        local_dir=\"/home/azureuser/cloudfiles/code/Users/randy.chase/appa_data\"\n",
        "    )\n",
        "    \n",
        "    print(f\"Data files downloaded to: {data_dir}\")\n",
        "    return Path(data_dir) / \"data\"\n",
        "\n",
        "def unscale_to_physical_units(decoded_samples, ae_model_path, device='cuda', data_dir=None):\n",
        "    \"\"\"\n",
        "    Unscale decoded samples back to original physical units.\n",
        "    \n",
        "    Args:\n",
        "        decoded_samples: Dictionary of decoded samples in standardized units\n",
        "        ae_model_path: Path to the autoencoder model directory\n",
        "        device: Device to use for computation\n",
        "        data_dir: Path to the data directory containing ERA5 statistics\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary of samples in original physical units\n",
        "    \"\"\"\n",
        "    from appa.data.transforms import StandardizeTransform\n",
        "    from appa.data.const import ERA5_VARIABLES, CONTEXT_VARIABLES, SUB_PRESSURE_LEVELS\n",
        "    from appa.config.hydra import compose\n",
        "    from pathlib import Path\n",
        "    \n",
        "    print(\"Loading ERA5 statistics for unscaling...\")\n",
        "    \n",
        "    # If no data directory provided, download from Hugging Face\n",
        "    if data_dir is None:\n",
        "        data_dir = download_appa_data_files()\n",
        "    else:\n",
        "        data_dir = Path(data_dir)\n",
        "    \n",
        "    # Set the path to the statistics file\n",
        "    stats_path = data_dir / \"stats_era5_1993-2021-1h-1440x721.zarr\"\n",
        "    \n",
        "    if not stats_path.exists():\n",
        "        raise FileNotFoundError(f\"Statistics file not found at {stats_path}\")\n",
        "    \n",
        "    print(f\"Using statistics file: {stats_path}\")\n",
        "    \n",
        "    # Load autoencoder config to get the correct pressure levels\n",
        "    ae_cfg = compose(Path(ae_model_path) / \"config.yaml\")\n",
        "    \n",
        "    # Define variables and levels (following render.py pattern)\n",
        "    variables_levels = {\n",
        "        \"state_variables\": ERA5_VARIABLES,\n",
        "        \"context_variables\": CONTEXT_VARIABLES,\n",
        "        \"levels\": SUB_PRESSURE_LEVELS if ae_cfg.train.sub_pressure_levels else None,\n",
        "    }\n",
        "    \n",
        "    # Create StandardizeTransform to get ERA5 statistics\n",
        "    st = StandardizeTransform(stats_path, **variables_levels)\n",
        "    \n",
        "    print(f\"ERA5 mean shape: {st.state_mean.shape}\")\n",
        "    print(f\"ERA5 std shape: {st.state_std.shape}\")\n",
        "    \n",
        "    # Keep statistics on CPU for memory efficiency\n",
        "    era5_mean = st.state_mean  # Already on CPU\n",
        "    era5_std = st.state_std    # Already on CPU\n",
        "    \n",
        "    print(f\"ERA5 mean device: {era5_mean.device}\")\n",
        "    print(f\"ERA5 std device: {era5_std.device}\")\n",
        "    \n",
        "    # Unscale each trajectory (all on CPU to avoid GPU memory issues)\n",
        "    unscaled_samples = {}\n",
        "    \n",
        "    for trajectory_size, trajectory_data in decoded_samples.items():\n",
        "        print(f\"Unscaling {trajectory_size}...\")\n",
        "        \n",
        "        unscaled_trajectory = {}\n",
        "        \n",
        "        for date_key, samples in trajectory_data.items():\n",
        "            print(f\"  Unscaling {date_key}...\")\n",
        "            \n",
        "            # Keep samples on CPU (they're already there from decoding)\n",
        "            # Apply inverse standardization: x_original = x_standardized * std + mean\n",
        "            # samples shape: (time, variables, lat, lon)\n",
        "            # era5_mean/std shape: (1, variables, 1, 1)\n",
        "            unscaled = samples * era5_std + era5_mean\n",
        "            \n",
        "            # Store directly (already on CPU)\n",
        "            unscaled_trajectory[date_key] = unscaled\n",
        "            \n",
        "            # Clean up memory\n",
        "            del samples, unscaled\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        unscaled_samples[trajectory_size] = unscaled_trajectory\n",
        "    \n",
        "    print(\"Unscaling completed!\")\n",
        "    return unscaled_samples\n",
        "\n",
        "# Example usage:\n",
        "# unscaled_samples = unscale_to_physical_units(decoded_samples, config['ae_model_path'], device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the unscaling to get samples in original physical units\n",
        "print(\"Starting unscaling process...\")\n",
        "unscaled_samples = unscale_to_physical_units(decoded_samples, config['ae_model_path'], device)\n",
        "\n",
        "# Print some statistics to verify the unscaling worked\n",
        "print(\"\\n=== UNSCALED SAMPLE STATISTICS ===\")\n",
        "for trajectory_size, trajectory_data in unscaled_samples.items():\n",
        "    print(f\"\\nTrajectory {trajectory_size}:\")\n",
        "    for date_key, samples in trajectory_data.items():\n",
        "        print(f\"  {date_key}: {samples.shape}\")\n",
        "        \n",
        "        # Show statistics for a few key variables\n",
        "        # Surface temperature (channel 0)\n",
        "        surface_temp = samples[:, 0, :, :]  # (time, lat, lon)\n",
        "        print(f\"    Surface temperature range: {surface_temp.min().item():.2f} to {surface_temp.max().item():.2f} K\")\n",
        "        \n",
        "        # Temperature at 850hPa (channel 16)\n",
        "        temp_850 = samples[:, 16, :, :]  # (time, lat, lon)\n",
        "        print(f\"    Temperature at 850hPa range: {temp_850.min().item():.2f} to {temp_850.max().item():.2f} K\")\n",
        "        \n",
        "        # Mean sea level pressure (channel 3)\n",
        "        mslp = samples[:, 3, :, :]  # (time, lat, lon)\n",
        "        print(f\"    Mean sea level pressure range: {mslp.min().item():.2f} to {mslp.max().item():.2f} Pa\")\n",
        "\n",
        "print(\"\\nUnscaling completed! Samples are now in original physical units.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic visualization of sample statistics\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, (trajectory_size, trajectory_data) in enumerate(decoded_samples.items()):\n",
        "    if idx >= 4:\n",
        "        break\n",
        "    \n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # Get all samples for this trajectory size\n",
        "    all_trajectory_samples = torch.cat(list(trajectory_data.values()), dim=0)\n",
        "    \n",
        "    # Plot mean and std across samples\n",
        "    mean_values = all_trajectory_samples.mean(dim=(0, 1))  # Mean across batch and time\n",
        "    std_values = all_trajectory_samples.std(dim=(0, 1))   # Std across batch and time\n",
        "    \n",
        "    ax.plot(mean_values.numpy(), label='Mean', alpha=0.7)\n",
        "    ax.fill_between(range(len(mean_values)), \n",
        "                    (mean_values - std_values).numpy(), \n",
        "                    (mean_values + std_values).numpy(), \n",
        "                    alpha=0.3, label='±1σ')\n",
        "    \n",
        "    ax.set_title(f'Trajectory {trajectory_size} - Sample Statistics')\n",
        "    ax.set_xlabel('Variable Index')\n",
        "    ax.set_ylabel('Value')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print sample information\n",
        "print(\"\\nGenerated Sample Summary:\")\n",
        "for trajectory_size, trajectory_data in decoded_samples.items():\n",
        "    print(f\"\\nTrajectory {trajectory_size}:\")\n",
        "    for date_key, samples in trajectory_data.items():\n",
        "        print(f\"  {date_key}: {samples.shape} samples\")\n",
        "        print(f\"    Mean: {samples.mean().item():.4f}\")\n",
        "        print(f\"    Std: {samples.std().item():.4f}\")\n",
        "        print(f\"    Min: {samples.min().item():.4f}\")\n",
        "        print(f\"    Max: {samples.max().item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "The generated samples can now be used for:\n",
        "\n",
        "1. **Rendering**: Use the rendering scripts to visualize the atmospheric states\n",
        "2. **Evaluation**: Compare against ground truth data for validation\n",
        "3. **Conditional Generation**: Use these as starting points for data assimilation\n",
        "4. **Forecasting**: Use as initial conditions for weather forecasting\n",
        "\n",
        "For more advanced usage, see the [Appa documentation](https://github.com/montefiore-sail/appa/wiki) for:\n",
        "- [Forecasting](https://github.com/montefiore-sail/appa/wiki/Forecasting)\n",
        "- [Reanalysis and Filtering](https://github.com/montefiore-sail/appa/wiki/Reanalysis-and-Filtering)\n",
        "- [Rendering States](https://github.com/montefiore-sail/appa/wiki/Rendering-States)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
